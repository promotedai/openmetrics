load("@rules_java//java:defs.bzl", "java_binary", "java_library")

DEPS = [
    "//pipeline/src/main/java/ai/promoted/metrics/logprocessor/common/error",
    "//pipeline/src/main/java/ai/promoted/metrics/logprocessor/common/error:avro",
    "//pipeline/src/main/java/ai/promoted/metrics/logprocessor/common/functions",
    "//pipeline/src/main/java/ai/promoted/metrics/logprocessor/common/job",
    "//pipeline/src/main/java/ai/promoted/metrics/logprocessor/common/job/paimon",
    "//pipeline/src/main/java/ai/promoted/metrics/logprocessor/common/model:avro",
    "//pipeline/src/main/java/ai/promoted/metrics/logprocessor/common/util",
    "@com_github_promotedai_schema_internal_git//proto/event:event_java_proto",
    "@com_google_protobuf//:protobuf_java",
    "@maven//:com_amazonaws_aws_java_sdk_glue",
    "@maven//:com_amazonaws_aws_java_sdk_core",
    "@maven//:com_amazonaws_aws_java_sdk_s3",
    "@maven//:com_google_code_findbugs_jsr305",
    "@maven//:com_google_guava_guava",
    "@maven//:com_google_protobuf_protobuf_java_util",
    "@maven//:info_picocli_picocli",
    "@maven_neverlink//:org_apache_flink_flink_core",
    "@maven_neverlink//:org_apache_flink_flink_java",
    "@maven_neverlink//:org_apache_flink_flink_streaming_java",
    "@maven_neverlink//:org_apache_flink_flink_table_api_java_uber",
    "@maven_neverlink//:org_apache_logging_log4j_log4j_api",
]

# For tests
java_library(
    name = "raw_output_job",
    srcs = glob(["*.java"]),
    visibility = ["//pipeline/src/main/java/ai/promoted/metrics/logprocessor:java_and_test"],
    deps = DEPS,
)

java_binary(
    name = "RawOutputJob",
    srcs = glob(["*.java"]),
    main_class = "ai.promoted.metrics.logprocessor.job.raw.RawOutputJob",
    visibility = ["//visibility:public"],
    deps = DEPS,
)

load("@io_bazel_rules_docker//container:container.bzl", "container_image")
load("@io_bazel_rules_docker//docker/util:run.bzl", "container_run_and_commit_layer")
load("@bazel_skylib//rules:common_settings.bzl", "string_flag")

string_flag(
    name = "image-arch",
    build_setting_default = "x86",
)

config_setting(
    name = "x86",
    flag_values = {
        ":image-arch": "x86",
    },
)

config_setting(
    name = "arm",
    flag_values = {
        ":image-arch": "arm",
    },
)

container_run_and_commit_layer(
    name = "os_upgrade",
    commands = [
        "apt-get update",
        "apt-get -y upgrade",
        "apt-get clean",
        "rm -rf /var/lib/apt/lists/*",
        "wget https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-kafka/1.17.1/flink-sql-connector-kafka-1.17.1.jar -O /opt/flink/lib/flink-sql-connector-kafka-1.17.1.jar",
    ],
    # official Flink image drops permissions in the entrypoint
    docker_run_flags = [
        "--entrypoint",
        "/usr/bin/env",
    ],
    image = select({
        ":arm": "@flink-arm//image",
        ":x86": "@flink//image",
        "//conditions:default": "@flink//image",
    }),
)

container_image(
    name = "RawOutputJob_image",
    base = select({
        ":arm": "@flink-arm//image",
        ":x86": "@flink//image",
        "//conditions:default": "@flink//image",
    }),
    cmd = ["RawOutputJob_deploy.jar"],
    data_path = "/pipeline/src/main/java/ai/promoted/metrics/logprocessor/job/raw",
    directory = "/opt",
    files = [
        ":RawOutputJob_deploy.jar",
    ],
    layers = [
        ":os_upgrade",
    ],
    stamp = True,
    # FLINK_VERSION = this needs updating when we increment the Flink version.
    symlinks = {"/opt/flink/plugins/s3-fs-hadoop/flink-s3-fs-hadoop-1.17.1.jar": "/opt/flink/opt/flink-s3-fs-hadoop-1.17.1.jar"},
)
