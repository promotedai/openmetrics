package ai.promoted.metrics.logprocessor.job.sqlrunner;

import ai.promoted.metrics.logprocessor.common.functions.base.SerializableConsumer;
import ai.promoted.metrics.logprocessor.common.job.BaseFlinkJob;
import ai.promoted.metrics.logprocessor.common.job.DirectFlatOutputKafkaSource;
import ai.promoted.metrics.logprocessor.common.job.DirectValidatedEventKafkaSource;
import ai.promoted.metrics.logprocessor.common.job.FlatOutputKafkaSegment;
import ai.promoted.metrics.logprocessor.common.job.FlinkSegment;
import ai.promoted.metrics.logprocessor.common.job.KafkaSegment;
import ai.promoted.metrics.logprocessor.common.job.KafkaSourceSegment;
import ai.promoted.metrics.logprocessor.common.job.ValidatedEventKafkaSegment;
import ai.promoted.metrics.logprocessor.common.table.MetricsTableCatalog;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.collect.ImmutableSet;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Properties;
import java.util.Set;
import java.util.regex.Pattern;
import java.util.stream.Collectors;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.types.Row;
import org.apache.flink.util.Preconditions;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;
import picocli.CommandLine;

@CommandLine.Command(
    name = "sqlrunner",
    mixinStandardHelpOptions = true,
    version = "alpha",
    description = "A generic Flink sql runner job")
public class SqlRunnerJob extends BaseFlinkJob {
  private static final Logger LOGGER = LogManager.getLogger(SqlRunnerJob.class);
  private static final String COMMENT_PATTERN = "(--.*)|((/\\*)+?[\\w\\W]+?(\\*/)+)";

  @CommandLine.Mixin public final KafkaSegment kafkaSegment = new KafkaSegment();

  @CommandLine.Mixin
  public final KafkaSourceSegment kafkaSourceSegment = new KafkaSourceSegment(this, kafkaSegment);

  @CommandLine.Mixin
  public final FlatOutputKafkaSegment flatOutputKafkaSegment =
      new FlatOutputKafkaSegment(this, kafkaSegment);

  @CommandLine.Mixin
  public DirectFlatOutputKafkaSource directFlatOutputKafkaSource =
      new DirectFlatOutputKafkaSource(kafkaSourceSegment, flatOutputKafkaSegment);

  @CommandLine.Mixin
  public final ValidatedEventKafkaSegment validatedEventKafkaSegment =
      new ValidatedEventKafkaSegment(this, kafkaSegment);

  @CommandLine.Mixin
  public DirectValidatedEventKafkaSource directValidatedEventKafkaSource =
      new DirectValidatedEventKafkaSource(kafkaSourceSegment, validatedEventKafkaSegment);

  @CommandLine.Option(
      names = {"--paimonCatalogPaths"},
      defaultValue = "",
      description = "A list of dfs catalog paths for Paimon tables.")
  public List<String> paimonCatalogPaths = new ArrayList<>();

  @CommandLine.Option(
      names = {"--platformEnv"},
      defaultValue = "",
      description = "The platform environment.")
  public String platformEnv;

  @CommandLine.Option(
      names = {"--sqlStatements"},
      defaultValue = "empty",
      description = "The sql statements to run.")
  public List<String> sqlStatements = new ArrayList<>();

  @CommandLine.Option(
      names = {"--fsAvroPathSchemas"},
      description =
          "Map of the FileSystem Avro path to schema class. The schema class can be Proto or Avro. Default=Empty")
  public Map<String, String> fsAvroPathSchemas = new HashMap<>();

  @CommandLine.Option(
      names = {"--options"},
      description = "Map of options to run the job. Default=Empty")
  public Map<String, String> options = new HashMap<>();

  private MetricsTableCatalog metricsTableCatalog;

  private SerializableConsumer<Row> resultConsumer = LOGGER::info; // Log the results by default

  public void setResultConsumer(SerializableConsumer<Row> resultConsumer) {
    this.resultConsumer = resultConsumer;
  }

  @Override
  public void startJob() throws Exception {
    Configuration configuration = new Configuration();
    Properties properties = new Properties();
    properties.putAll(options);
    configuration.addAllToProperties(properties);
    StreamExecutionEnvironment env =
        StreamExecutionEnvironment.getExecutionEnvironment(configuration);
    this.disableAutoGeneratedUIDs = false;
    configureExecutionEnvironment(env, parallelism, maxParallelism);
    EnvironmentSettings settings =
        EnvironmentSettings.newInstance().withConfiguration(configuration).build();
    startSqlRunner(env, settings);
  }

  @VisibleForTesting
  void startSqlRunner(StreamExecutionEnvironment env, EnvironmentSettings environmentSettings) {
    StreamTableEnvironment tableEnvironment =
        StreamTableEnvironment.create(env, environmentSettings);
    metricsTableCatalog =
        new MetricsTableCatalog(
            platformEnv, this.getJobLabel(), this.toKafkaConsumerGroupId("sqlRunner"));
    if (!paimonCatalogPaths.isEmpty()) {
      metricsTableCatalog.setPaimonCatalogPaths(paimonCatalogPaths);
    }
    metricsTableCatalog.addFlatOutputSourceProvider(
        DirectValidatedEventKafkaSource.DIRECT_VALIDATED_RO_DB_PREFIX, directFlatOutputKafkaSource);
    metricsTableCatalog.addValidatedSourceProvider(
        DirectValidatedEventKafkaSource.DIRECT_VALIDATED_RO_DB_PREFIX,
        directValidatedEventKafkaSource);
    metricsTableCatalog.setFsAvroPathSchemas(fsAvroPathSchemas);
    List<String> registeredTables =
        metricsTableCatalog.registerMetricsTables(env, tableEnvironment);
    LOGGER.info("Registered tables: {}", registeredTables);
    LOGGER.info("Sql statements: {}", sqlStatements);
    List<String> statementsToRun = getSqlStatementsToRun();
    LOGGER.info("Sql statements to run {}", statementsToRun);

    for (String sql : statementsToRun) {
      LOGGER.info("Execute {}", sql);
      tableEnvironment.executeSql(sql).collect().forEachRemaining(resultConsumer);
    }
  }

  private List<String> getSqlStatementsToRun() {
    Pattern pattern = Pattern.compile(COMMENT_PATTERN);
    return sqlStatements.stream()
        .map(String::trim)
        .filter(s -> !pattern.matcher(s).matches())
        .collect(Collectors.toList());
  }

  @Override
  protected String getDefaultBaseJobName() {
    return "sql-runner";
  }

  @Override
  public Set<FlinkSegment> getInnerFlinkSegments() {
    return ImmutableSet.of(
        kafkaSegment,
        kafkaSourceSegment,
        flatOutputKafkaSegment,
        directFlatOutputKafkaSource,
        validatedEventKafkaSegment,
        directValidatedEventKafkaSource);
  }

  @Override
  public void validateArgs() {
    super.validateArgs();
    Preconditions.checkArgument(!sqlStatements.isEmpty(), "Sql statement list can't be empty!");
  }

  public static void main(String[] args) {
    executeMain(new SqlRunnerJob(), args);
  }
}
