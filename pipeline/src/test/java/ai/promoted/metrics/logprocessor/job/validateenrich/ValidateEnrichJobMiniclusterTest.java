package ai.promoted.metrics.logprocessor.job.validateenrich;

import static ai.promoted.metrics.logprocessor.common.testing.AvroAsserts.assertAvroFiles;
import static ai.promoted.metrics.logprocessor.common.testing.AvroAsserts.assertAvroParquetFiles;
import static ai.promoted.metrics.logprocessor.common.testing.AvroAsserts.loadAvroParquetRecords;
import static ai.promoted.metrics.logprocessor.common.testing.AvroProtoUtils.toFixedAvroGenericRecords;
import static ai.promoted.metrics.logprocessor.common.testing.FileAsserts.assertPartFilesExist;
import static com.google.common.truth.Truth.assertThat;

import ai.promoted.metrics.common.Field;
import ai.promoted.metrics.common.LogUserUser;
import ai.promoted.metrics.common.RecordType;
import ai.promoted.metrics.error.ErrorType;
import ai.promoted.metrics.error.ValidationError;
import ai.promoted.metrics.error.ValidationError.Builder;
import ai.promoted.metrics.logprocessor.common.fakedatagenerator.LogRequestFactory;
import ai.promoted.metrics.logprocessor.common.job.S3FileOutput;
import ai.promoted.metrics.logprocessor.common.job.S3Segment;
import ai.promoted.metrics.logprocessor.common.job.testing.BaseJobMiniclusterTest;
import ai.promoted.metrics.logprocessor.common.job.testing.MockValidatedEventSink;
import ai.promoted.metrics.logprocessor.common.testing.MiniClusterExtension;
import ai.promoted.metrics.logprocessor.common.util.TableUtil;
import ai.promoted.metrics.logprocessor.common.util.TrackingUtil;
import ai.promoted.proto.common.AnonUserRetainedUser;
import ai.promoted.proto.common.RetainedUser;
import ai.promoted.proto.common.UserInfo;
import ai.promoted.proto.delivery.DeliveryLog;
import ai.promoted.proto.event.Action;
import ai.promoted.proto.event.ActionType;
import ai.promoted.proto.event.CohortMembership;
import ai.promoted.proto.event.Diagnostics;
import ai.promoted.proto.event.Impression;
import ai.promoted.proto.event.LogRequest;
import ai.promoted.proto.event.View;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import java.io.File;
import java.time.Duration;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Set;
import java.util.function.Supplier;
import java.util.stream.Collectors;
import org.apache.avro.generic.GenericRecord;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.TableEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.types.Row;
import org.apache.flink.util.Preconditions;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.testcontainers.junit.jupiter.Testcontainers;

/** Minicluster test for ValidateEnrichJob. */
@ExtendWith(MiniClusterExtension.class)
@Testcontainers
public class ValidateEnrichJobMiniclusterTest extends BaseJobMiniclusterTest<ValidateEnrichJob> {
  private static final String DATABASE_NAME = "raw_db";
  // GMT: Thursday, October 1, 2020 11:49:11 PM
  private static final long EVENT_API_TIMESTAMP = 1601596151000L;
  private static final String DT = "2020-10-01";
  private static final String HOUR = "23";

  private static final List<LogRequest> createLogRequests() {
    return LogRequestFactory.createLogRequests(
        LogRequestFactory.createLogRequestOptionsBuilder(
                EVENT_API_TIMESTAMP, LogRequestFactory.DetailLevel.PARTIAL, 1)
            .setLogTimestamp(true));
  }

  @Override
  protected ValidateEnrichJob createJob() {
    ValidateEnrichJob job = new ValidateEnrichJob();
    job.databaseName = DATABASE_NAME;
    StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
    job.setTableEnv(tEnv);
    job.checkpointInterval = Duration.ofSeconds(1);
    job.checkpointTimeout = Duration.ofSeconds(60);
    job.disableAutoGeneratedUIDs = false;
    job.paimonSegment.paimonBasePath = tempDir.getAbsolutePath() + "/paimon/";
    job.paimonSegment.paimonCatalogName = "raw-tables";
    job.paimonSegment.paimonDefaultDatabaseName = DATABASE_NAME;
    job.anonymizerSegment.staticKeyAnonymizer = new HashMap<>();
    job.anonymizerSegment.staticKeyAnonymizer.put("retained_user_id", "testsalttestsal");

    job.configureExecutionEnvironment(env, 1, 0);
    env.getCheckpointConfig().enableUnalignedCheckpoints(false);
    TrackingUtil.processingTimeSupplier = () -> 0L;
    return job;
  }

  // This data has logUserId set but not anonUserId.
  @Test
  void test_v1Migration() throws Exception {
    ValidateEnrichJob job = createJob();
    job.kafkaSourceSegment.startFromEarliest = true;

    S3Segment s3 = new S3Segment(job);
    s3.rootPath = tempDir.getAbsolutePath();
    S3FileOutput s3FileOutput = new S3FileOutput(job, s3);
    job.validatedEventSink = new MockValidatedEventSink(s3FileOutput);
    List<LogRequest> logRequests = createLogRequests();

    job.startValidateAndEnrichJob(
        job.metricsApiKafkaSource.splitSources(
            fromItems(env, "logRequest", logRequests, LogRequest::getTiming)));

    waitForDone(env.execute("validate-enrich"));

    // Our test Kafka infra has issues so we'll write to files.
    File kafkaOutput = new File(tempDir, "kafka");
    assertThat(ImmutableSet.copyOf(kafkaOutput.list()))
        .containsExactly(
            "metrics.default.cohort-membership",
            "metrics.default.view",
            "metrics.default.delivery-log",
            "metrics.default.impression",
            "metrics.default.action",
            "metrics.default.diagnostics");

    String path = String.format("kafka/metrics.default.cohort-membership/dt=%s/hour=%s", DT, HOUR);
    CohortMembership.Builder expectedCohortMembershipBuilder =
        getOnlyCohortMembership(logRequests).toBuilder();
    copyLogToAnonUserId(expectedCohortMembershipBuilder.getUserInfoBuilder());
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedCohortMembershipBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "cohort-membership");

    path = String.format("kafka/metrics.default.view/dt=%s/hour=%s", DT, HOUR);
    View.Builder expectedViewBuilder = getOnlyView(logRequests).toBuilder();
    copyLogToAnonUserId(expectedViewBuilder.getUserInfoBuilder());
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedViewBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "view");

    path = String.format("kafka/metrics.default.delivery-log/dt=%s/hour=%s", DT, HOUR);
    DeliveryLog.Builder expectedDeliveryLogBuilder = getOnlyDeliveryLog(logRequests).toBuilder();
    copyLogToAnonUserId(expectedDeliveryLogBuilder.getRequestBuilder().getUserInfoBuilder());
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedDeliveryLogBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "delivery-log");

    path = String.format("kafka/metrics.default.impression/dt=%s/hour=%s", DT, HOUR);
    Impression.Builder expectedImpressionBuilder = getOnlyImpression(logRequests).toBuilder();
    copyLogToAnonUserId(expectedImpressionBuilder.getUserInfoBuilder());
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedImpressionBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "impression");

    path = String.format("kafka/metrics.default.action/dt=%s/hour=%s", DT, HOUR);
    Action.Builder expectedActionBuilder = getOnlyAction(logRequests).toBuilder();
    copyLogToAnonUserId(expectedActionBuilder.getUserInfoBuilder());
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedActionBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "action");

    path = String.format("kafka/metrics.default.diagnostics/dt=%s/hour=%s", DT, HOUR);
    Diagnostics.Builder expectedDiagnosticsBuilder = getOnlyDiagnostics(logRequests).toBuilder();
    copyLogToAnonUserId(expectedDiagnosticsBuilder.getUserInfoBuilder());
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedDiagnosticsBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "diagnostics");
  }

  // This data has logUserId set but not anonUserId.
  @Test
  void test_v2() throws Exception {
    ValidateEnrichJob job = createJob();
    job.kafkaSourceSegment.startFromEarliest = true;

    S3Segment s3 = new S3Segment(job);
    s3.rootPath = tempDir.getAbsolutePath();
    S3FileOutput s3FileOutput = new S3FileOutput(job, s3);
    job.validatedEventSink = new MockValidatedEventSink(s3FileOutput);
    List<LogRequest> logRequests =
        LogRequestFactory.createLogRequests(
            LogRequestFactory.createLogRequestOptionsBuilder(
                    EVENT_API_TIMESTAMP, LogRequestFactory.DetailLevel.PARTIAL, 1)
                .setWriteAnonUserId(true)
                .setWriteLogUserId(false)
                .setLogTimestamp(true));

    job.startValidateAndEnrichJob(
        job.metricsApiKafkaSource.splitSources(
            fromItems(env, "logRequest", logRequests, LogRequest::getTiming)));
    waitForDone(env.execute("validate-enrich"));

    // Our test Kafka infra has issues so we'll write to files.
    File kafkaOutput = new File(tempDir, "kafka");
    assertThat(ImmutableSet.copyOf(kafkaOutput.list()))
        .containsExactly(
            "metrics.default.cohort-membership",
            "metrics.default.view",
            "metrics.default.delivery-log",
            "metrics.default.impression",
            "metrics.default.action",
            "metrics.default.diagnostics");

    String path = String.format("kafka/metrics.default.cohort-membership/dt=%s/hour=%s", DT, HOUR);
    CohortMembership.Builder expectedCohortMembershipBuilder =
        getOnlyCohortMembership(logRequests).toBuilder();
    copyAnonToLogUserId(expectedCohortMembershipBuilder.getUserInfoBuilder());
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedCohortMembershipBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "cohort-membership");

    path = String.format("kafka/metrics.default.view/dt=%s/hour=%s", DT, HOUR);
    View.Builder expectedViewBuilder = getOnlyView(logRequests).toBuilder();
    copyAnonToLogUserId(expectedViewBuilder.getUserInfoBuilder());
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedViewBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "view");

    path = String.format("kafka/metrics.default.delivery-log/dt=%s/hour=%s", DT, HOUR);
    DeliveryLog.Builder expectedDeliveryLogBuilder = getOnlyDeliveryLog(logRequests).toBuilder();
    copyAnonToLogUserId(expectedDeliveryLogBuilder.getRequestBuilder().getUserInfoBuilder());
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedDeliveryLogBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "delivery-log");

    path = String.format("kafka/metrics.default.impression/dt=%s/hour=%s", DT, HOUR);
    Impression.Builder expectedImpressionBuilder = getOnlyImpression(logRequests).toBuilder();
    copyAnonToLogUserId(expectedImpressionBuilder.getUserInfoBuilder());
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedImpressionBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "impression");

    path = String.format("kafka/metrics.default.action/dt=%s/hour=%s", DT, HOUR);
    Action.Builder expectedActionBuilder = getOnlyAction(logRequests).toBuilder();
    copyAnonToLogUserId(expectedActionBuilder.getUserInfoBuilder());
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedActionBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "action");

    path = String.format("kafka/metrics.default.diagnostics/dt=%s/hour=%s", DT, HOUR);
    Diagnostics.Builder expectedDiagnosticsBuilder = getOnlyDiagnostics(logRequests).toBuilder();
    copyAnonToLogUserId(expectedDiagnosticsBuilder.getUserInfoBuilder());
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedDiagnosticsBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "diagnostics");
  }

  @Test
  void customActionTypeMap() throws Exception {
    ValidateEnrichJob job = createJob();
    job.kafkaSourceSegment.startFromEarliest = true;
    job.customActionTypeMap.put("custom purchase", ActionType.PURCHASE);

    S3Segment s3 = new S3Segment(job);
    s3.rootPath = tempDir.getAbsolutePath();
    S3FileOutput s3FileOutput = new S3FileOutput(job, s3);
    job.validatedEventSink = new MockValidatedEventSink(s3FileOutput);
    List<LogRequest> logRequests =
        LogRequestFactory.createLogRequests(
            LogRequestFactory.createLogRequestOptionsBuilder(
                    EVENT_API_TIMESTAMP, LogRequestFactory.DetailLevel.PARTIAL, 1)
                .setWriteAnonUserId(true)
                .setWriteLogUserId(false)
                .setLogTimestamp(true));

    logRequests =
        logRequests.stream()
            .map(
                logRequest -> {
                  if (logRequest.getActionCount() > 0) {
                    LogRequest.Builder builder = logRequest.toBuilder();
                    builder
                        .getActionBuilderList()
                        .forEach(action -> action.setCustomActionType("custom purchase"));
                    return builder.build();
                  } else {
                    return logRequest;
                  }
                })
            .collect(Collectors.toList());

    job.startValidateAndEnrichJob(
        job.metricsApiKafkaSource.splitSources(
            fromItems(env, "logRequest", logRequests, LogRequest::getTiming)));
    waitForDone(env.execute("validate-enrich"));

    // Our test Kafka infra has issues so we'll write to files.
    File kafkaOutput = new File(tempDir, "kafka");
    assertThat(ImmutableSet.copyOf(kafkaOutput.list())).contains("metrics.default.action");

    String path = String.format("kafka/metrics.default.action/dt=%s/hour=%s", DT, HOUR);
    Action.Builder expectedActionBuilder = getOnlyAction(logRequests).toBuilder();
    expectedActionBuilder.setActionType(ActionType.PURCHASE);
    copyAnonToLogUserId(expectedActionBuilder.getUserInfoBuilder());
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedActionBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "action");
  }

  @Test
  void removeBadCustomActionType() throws Exception {
    ValidateEnrichJob job = createJob();
    job.kafkaSourceSegment.startFromEarliest = true;
    job.customActionTypeMap.put("custom purchase", ActionType.PURCHASE);
    // Pick a far date in the future.
    job.removeEventsSegment.customActionTypeToRemoveEventApiTimestampRange.put(
        "custom purchase", "0,2000000000000");

    S3Segment s3 = new S3Segment(job);
    s3.rootPath = tempDir.getAbsolutePath();
    S3FileOutput s3FileOutput = new S3FileOutput(job, s3);
    job.validatedEventSink = new MockValidatedEventSink(s3FileOutput);
    List<LogRequest> logRequests =
        LogRequestFactory.createLogRequests(
            LogRequestFactory.createLogRequestOptionsBuilder(
                    EVENT_API_TIMESTAMP, LogRequestFactory.DetailLevel.PARTIAL, 1)
                .setWriteAnonUserId(true)
                .setWriteLogUserId(false)
                .setLogTimestamp(true));

    logRequests =
        logRequests.stream()
            .map(
                logRequest -> {
                  if (logRequest.getActionCount() > 0) {
                    LogRequest.Builder builder = logRequest.toBuilder();
                    builder
                        .getActionBuilderList()
                        .forEach(action -> action.setCustomActionType("custom purchase"));
                    return builder.build();
                  } else {
                    return logRequest;
                  }
                })
            .collect(Collectors.toList());

    job.startValidateAndEnrichJob(
        job.metricsApiKafkaSource.splitSources(
            fromItems(env, "logRequest", logRequests, LogRequest::getTiming)));
    waitForDone(env.execute("validate-enrich"));

    // Our test Kafka infra has issues so we'll write to files.
    File kafkaOutput = new File(tempDir, "kafka");
    assertThat(ImmutableSet.copyOf(kafkaOutput.list())).doesNotContain("metrics.default.action");
  }

  @Test
  void anonymizeUserId() throws Exception {
    ValidateEnrichJob job = createJob();
    job.kafkaSourceSegment.startFromEarliest = true;
    job.anonymizerSegment.staticKeyAnonymizer.put("anon_user_id", "anonsalttestsal");

    S3Segment s3 = new S3Segment(job);
    s3.rootPath = tempDir.getAbsolutePath();
    S3FileOutput s3FileOutput = new S3FileOutput(job, s3);
    job.validatedEventSink = new MockValidatedEventSink(s3FileOutput);
    List<LogRequest> logRequests =
        LogRequestFactory.createLogRequests(
            LogRequestFactory.createLogRequestOptionsBuilder(
                    EVENT_API_TIMESTAMP, LogRequestFactory.DetailLevel.PARTIAL, 1)
                .setWriteUserId(true)
                .setWriteAnonUserId(false)
                .setWriteLogUserId(false)
                .setLogTimestamp(true));

    job.startValidateAndEnrichJob(
        job.metricsApiKafkaSource.splitSources(
            fromItems(env, "logRequest", logRequests, LogRequest::getTiming)));
    waitForDone(env.execute("validate-enrich"));

    // Our test Kafka infra has issues so we'll write to files.
    File kafkaOutput = new File(tempDir, "kafka");
    assertThat(ImmutableSet.copyOf(kafkaOutput.list()))
        .containsExactly(
            "metrics.default.action",
            "metrics.default.anon-user-retained-user-event",
            "metrics.default.cohort-membership",
            "metrics.default.delivery-log",
            "metrics.default.diagnostics",
            "metrics.default.impression",
            "metrics.default.log-user-user-event",
            "metrics.default.retained-user",
            "metrics.default.view");

    String path = String.format("kafka/metrics.default.cohort-membership/dt=%s/hour=%s", DT, HOUR);
    CohortMembership.Builder expectedCohortMembershipBuilder =
        getOnlyCohortMembership(logRequests).toBuilder();
    // Hash for "userId1".
    String anonUserId = "4ea3f2181b941e8653f5def4251f73ef";
    expectedCohortMembershipBuilder
        .getUserInfoBuilder()
        .setAnonUserId(anonUserId)
        .setLogUserId(anonUserId)
        .setRetainedUserId(anonUserId)
        .clearUserId();
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedCohortMembershipBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "cohort-membership");

    path = String.format("kafka/metrics.default.view/dt=%s/hour=%s", DT, HOUR);
    View.Builder expectedViewBuilder = getOnlyView(logRequests).toBuilder();
    expectedViewBuilder
        .getUserInfoBuilder()
        .setAnonUserId(anonUserId)
        .setLogUserId(anonUserId)
        .setRetainedUserId(anonUserId)
        .clearUserId();
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedViewBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "view");

    path = String.format("kafka/metrics.default.delivery-log/dt=%s/hour=%s", DT, HOUR);
    DeliveryLog.Builder expectedDeliveryLogBuilder = getOnlyDeliveryLog(logRequests).toBuilder();
    copyAnonToLogUserId(expectedDeliveryLogBuilder.getRequestBuilder().getUserInfoBuilder());
    expectedDeliveryLogBuilder
        .getRequestBuilder()
        .getUserInfoBuilder()
        .setAnonUserId(anonUserId)
        .setLogUserId(anonUserId)
        .setRetainedUserId(anonUserId)
        .clearUserId();
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedDeliveryLogBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "delivery-log");

    path = String.format("kafka/metrics.default.impression/dt=%s/hour=%s", DT, HOUR);
    Impression.Builder expectedImpressionBuilder = getOnlyImpression(logRequests).toBuilder();
    expectedImpressionBuilder
        .getUserInfoBuilder()
        .setAnonUserId(anonUserId)
        .setLogUserId(anonUserId)
        .setRetainedUserId(anonUserId)
        .clearUserId();
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedImpressionBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "impression");

    path = String.format("kafka/metrics.default.action/dt=%s/hour=%s", DT, HOUR);
    Action.Builder expectedActionBuilder = getOnlyAction(logRequests).toBuilder();
    expectedActionBuilder
        .getUserInfoBuilder()
        .setAnonUserId(anonUserId)
        .setLogUserId(anonUserId)
        .setRetainedUserId(anonUserId)
        .clearUserId();
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedActionBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "action");

    path = String.format("kafka/metrics.default.diagnostics/dt=%s/hour=%s", DT, HOUR);
    Diagnostics.Builder expectedDiagnosticsBuilder = getOnlyDiagnostics(logRequests).toBuilder();
    expectedDiagnosticsBuilder
        .getUserInfoBuilder()
        .setAnonUserId(anonUserId)
        .setLogUserId(anonUserId)
        .setRetainedUserId(anonUserId)
        .clearUserId();
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedDiagnosticsBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "diagnostics");

    path = String.format("kafka/metrics.default.retained-user/dt=%s/hour=%s", DT, HOUR);
    assertAvroFiles(
        toFixedAvroGenericRecords(
            listOf(
                RetainedUser.newBuilder()
                    .setPlatformId(1L)
                    .setUserId("userId1")
                    .setRetainedUserId(anonUserId)
                    .setCreateEventApiTimeMillis(1601596151000L)
                    .setProcessTimeMillis(0L)
                    .build())),
        assertPartFilesExist(new File(tempDir, path)),
        "retained-user");

    path = String.format("kafka/metrics.default.log-user-user-event/dt=%s/hour=%s", DT, HOUR);
    assertAvroParquetFiles(
        listOf(
            LogUserUser.newBuilder()
                .setPlatformId(1L)
                .setLogUserId(anonUserId)
                .setUserId("userId1")
                .setEventTimeMillis(1601596151000L)
                .build()),
        assertPartFilesExist(new File(tempDir, path)),
        "log-user-user");

    path =
        String.format(
            "kafka/metrics.default.anon-user-retained-user-event/dt=%s/hour=%s", DT, HOUR);
    assertAvroFiles(
        toFixedAvroGenericRecords(
            listOf(
                AnonUserRetainedUser.newBuilder()
                    .setPlatformId(1L)
                    .setAnonUserId(anonUserId)
                    .setRetainedUserId(anonUserId)
                    .setEventTimeMillis(1601596151000L)
                    .build())),
        assertPartFilesExist(new File(tempDir, path)),
        "anon-user-retained-user");
  }

  // Invalid inputs because neither anonUserId or logUserId are set.
  @Test
  void test_invalidInputs() throws Exception {
    ValidateEnrichJob job = createJob();
    job.kafkaSourceSegment.startFromEarliest = true;

    S3Segment s3 = new S3Segment(job);
    s3.rootPath = tempDir.getAbsolutePath();
    S3FileOutput s3FileOutput = new S3FileOutput(job, s3);
    job.validatedEventSink = new MockValidatedEventSink(s3FileOutput);
    List<LogRequest> logRequests =
        LogRequestFactory.createLogRequests(
            LogRequestFactory.createLogRequestOptionsBuilder(
                    EVENT_API_TIMESTAMP, LogRequestFactory.DetailLevel.PARTIAL, 1)
                .setWriteAnonUserId(false)
                .setWriteLogUserId(false)
                .setLogTimestamp(true));

    job.startValidateAndEnrichJob(
        job.metricsApiKafkaSource.splitSources(
            fromItems(env, "logRequest", logRequests, LogRequest::getTiming)));
    waitForDone(env.execute("validate-enrich"));

    // Our test Kafka infra has issues so we'll write to files.
    File kafkaOutput = new File(tempDir, "kafka");
    assertThat(ImmutableSet.copyOf(kafkaOutput.list()))
        .containsExactly(
            "metrics.default.invalid-action",
            "metrics.default.invalid-cohort-membership",
            "metrics.default.invalid-delivery-log",
            "metrics.default.invalid-diagnostics",
            "metrics.default.invalid-impression",
            "metrics.default.invalid-view",
            "metrics.default.validation-error");

    String path = String.format("kafka/metrics.default.validation-error/dt=%s/hour=%s", DT, HOUR);
    Supplier<Builder> newErrorBuilder =
        () ->
            ValidationError.newBuilder()
                .setPlatformId(1L)
                .setErrorType(ErrorType.MISSING_FIELD)
                .setAnonUserId("")
                .setField(Field.ANON_USER_ID)
                .setTiming(
                    ai.promoted.metrics.common.Timing.newBuilder()
                        .setClientLogTimestamp(1601596151000L)
                        .setEventApiTimestamp(1601596151000L)
                        .setLogTimestamp(1601596151000L)
                        .build());

    ImmutableSet<GenericRecord> validationErrors =
        loadAvroParquetRecords(assertPartFilesExist(new File(tempDir, path)));
    assertThat(validationErrors).hasSize(6);
    assertThat(validationErrors)
        .contains(
            newErrorBuilder
                .get()
                .setRecordType(RecordType.COHORT_MEMBERSHIP)
                .setCohortMembershipId("cccccccc-cccc-cccc-0000-000000000001")
                .build());
    assertThat(validationErrors)
        .contains(
            newErrorBuilder
                .get()
                .setRecordType(RecordType.VIEW)
                .setViewId("22222222-2222-2222-0000-000000000001")
                .build());
    assertThat(validationErrors)
        .contains(
            newErrorBuilder
                .get()
                .setRecordType(RecordType.DELIVERY_LOG)
                .setViewId("22222222-2222-2222-0000-000000000001")
                .setRequestId("33333333-3333-3333-0000-000000000001")
                .build());
    assertThat(validationErrors)
        .contains(
            newErrorBuilder
                .get()
                .setRecordType(RecordType.IMPRESSION)
                .setResponseInsertionId("44444444-4444-4444-0000-000000000001")
                .setImpressionId("55555555-5555-5555-0000-000000000001")
                .build());
    assertThat(validationErrors)
        .contains(
            newErrorBuilder
                .get()
                .setRecordType(RecordType.ACTION)
                .setImpressionId("55555555-5555-5555-0000-000000000001")
                .setActionId("66666666-6666-6666-0000-000000000001")
                .build());
    assertThat(validationErrors)
        .contains(newErrorBuilder.get().setRecordType(RecordType.DIAGNOSTICS).build());

    path = String.format("kafka/metrics.default.invalid-cohort-membership/dt=%s/hour=%s", DT, HOUR);
    CohortMembership.Builder expectedCohortMembershipBuilder =
        getOnlyCohortMembership(logRequests).toBuilder();
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedCohortMembershipBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "invalid-cohort-membership");

    path = String.format("kafka/metrics.default.invalid-view/dt=%s/hour=%s", DT, HOUR);
    View.Builder expectedViewBuilder = getOnlyView(logRequests).toBuilder();
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedViewBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "invalid-view");

    path = String.format("kafka/metrics.default.invalid-delivery-log/dt=%s/hour=%s", DT, HOUR);
    DeliveryLog.Builder expectedDeliveryLogBuilder = getOnlyDeliveryLog(logRequests).toBuilder();
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedDeliveryLogBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "invalid-delivery-log");

    path = String.format("kafka/metrics.default.invalid-impression/dt=%s/hour=%s", DT, HOUR);
    Impression.Builder expectedImpressionBuilder = getOnlyImpression(logRequests).toBuilder();
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedImpressionBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "invalid-impression");

    path = String.format("kafka/metrics.default.invalid-action/dt=%s/hour=%s", DT, HOUR);
    Action.Builder expectedActionBuilder = getOnlyAction(logRequests).toBuilder();
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedActionBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "invalid-action");

    path = String.format("kafka/metrics.default.invalid-diagnostics/dt=%s/hour=%s", DT, HOUR);
    Diagnostics.Builder expectedDiagnosticsBuilder = getOnlyDiagnostics(logRequests).toBuilder();
    assertAvroFiles(
        toFixedAvroGenericRecords(listOf(expectedDiagnosticsBuilder.build())),
        assertPartFilesExist(new File(tempDir, path)),
        "invalid-diagnostics");
  }

  @Test
  void authUserId_noPreviousPaimonRecords_hasAnonUserId() throws Exception {
    ValidateEnrichJob job = createJob();
    job.kafkaSourceSegment.startFromEarliest = true;

    S3Segment s3 = new S3Segment(job);
    s3.rootPath = tempDir.getAbsolutePath();
    S3FileOutput s3FileOutput = new S3FileOutput(job, s3);
    job.validatedEventSink = new MockValidatedEventSink(s3FileOutput);
    runAuthUserIdTest(
        job,
        ImmutableSet.of(
            "metrics.default.action",
            "metrics.default.anon-user-retained-user-event",
            "metrics.default.log-user-user-event",
            "metrics.default.retained-user"),
        "anonuserid1");

    String path = String.format("kafka/metrics.default.retained-user/dt=%s/hour=%s", DT, HOUR);
    assertAvroFiles(
        toFixedAvroGenericRecords(
            listOf(
                RetainedUser.newBuilder()
                    .setPlatformId(1L)
                    .setUserId("userId1")
                    .setRetainedUserId("anonuserid1")
                    .setCreateEventApiTimeMillis(1601596151000L)
                    .setProcessTimeMillis(0L)
                    .build())),
        assertPartFilesExist(new File(tempDir, path)),
        "retained-user");

    // Verify the table is created.
    String catalogPath = new File(tempDir, "paimon/").getAbsolutePath();
    String createCatalogSql =
        String.format(
            "CREATE CATALOG paimon WITH ('warehouse'='%s', 'type'='paimon')", catalogPath);
    EnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build();
    TableEnvironment tableEnv = TableEnvironment.create(settings);
    tableEnv.executeSql(createCatalogSql);
    String dbName = TableUtil.getLabeledDatabaseName(DATABASE_NAME, job.getJobLabel());
    String createDatabaseSql = String.format("CREATE DATABASE IF NOT EXISTS `paimon`.`%s`", dbName);
    tableEnv.executeSql(createDatabaseSql);
    String fullTableName = TableUtil.fullTableName("paimon", dbName, "retained_user");
    String createTableSql = ValidateEnrichJob.getCreateRetainedUserPaimonTableSql(fullTableName);
    tableEnv.executeSql(createTableSql);
    Table table = tableEnv.sqlQuery(String.format("SELECT * FROM %s", fullTableName));
    Iterator<Row> result = table.execute().collect();
    // TODO - why is process timestamp null?  I'd expect zero.
    String expectedDeliveryLogStr = "+I[1, userId1, anonuserid1, 1601596151000, null]";
    assertThat(result.next().toString()).isEqualTo(expectedDeliveryLogStr);
  }

  // This test seems flaky.  Sometimes the Paimon lookup doesn't fetch a record.
  @Test
  void authUserId_hasPaimonRecords() throws Exception {
    ValidateEnrichJob job = createJob();
    job.kafkaSourceSegment.startFromEarliest = true;

    // Create RetainedUser Paimon table.
    String catalogPath = new File(tempDir, "paimon/").getAbsolutePath();
    String createCatalogSql =
        String.format(
            "CREATE CATALOG paimon WITH ('warehouse'='%s', 'type'='paimon')", catalogPath);
    EnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build();
    TableEnvironment tableEnv = TableEnvironment.create(settings);
    tableEnv.executeSql(createCatalogSql);
    String dbName = DATABASE_NAME;
    // String dbName = job.getLabeledDatabaseName(DATABASE_NAME);
    String createDatabaseSql = String.format("CREATE DATABASE IF NOT EXISTS `paimon`.`%s`", dbName);
    tableEnv.executeSql(createDatabaseSql);
    String fullTableName = TableUtil.fullTableName("paimon", dbName, "retained_user");
    String createTableSql = ValidateEnrichJob.getCreateRetainedUserPaimonTableSql(fullTableName);
    tableEnv.executeSql(createTableSql);
    // The Paimon record is from
    // 1601546400000 = GMT: Thursday, October 1, 2020 10:00:00 AM
    // These events are an hour later.
    // 1601596151001 GMT: Thursday, October 1, 2020 11:49:11 PM
    String insertTestDataSql =
        String.format(
            "INSERT INTO %s VALUES (1, 'userId1', 'previousRetainedUserId', 1601546400000, 1601546401000, 0)",
            fullTableName);
    tableEnv.executeSql(insertTestDataSql);

    S3Segment s3 = new S3Segment(job);
    s3.rootPath = tempDir.getAbsolutePath();
    S3FileOutput s3FileOutput = new S3FileOutput(job, s3);
    job.validatedEventSink = new MockValidatedEventSink(s3FileOutput);
    runAuthUserIdTest(
        job,
        ImmutableSet.of(
            "metrics.default.anon-user-retained-user-event",
            "metrics.default.log-user-user-event",
            "metrics.default.action"),
        "previousRetainedUserId");
  }

  @Test
  void authUserId_noPreviousPaimonRecords_noAnonUserId() throws Exception {
    ValidateEnrichJob job = createJob();
    job.kafkaSourceSegment.startFromEarliest = true;
    // These flags are currently needed to allow records without anonUserId.
    // We'll soon allow these records normally.
    job.requireAnonUserId = false;

    S3Segment s3 = new S3Segment(job);
    s3.rootPath = tempDir.getAbsolutePath();
    S3FileOutput s3FileOutput = new S3FileOutput(job, s3);
    job.validatedEventSink = new MockValidatedEventSink(s3FileOutput);

    Action.Builder actionBuilder =
        Action.newBuilder()
            .setPlatformId(1L)
            .setTiming(
                ai.promoted.proto.common.Timing.newBuilder()
                    .setEventApiTimestamp(EVENT_API_TIMESTAMP))
            .setContentId("content1");
    Action action1 =
        actionBuilder
            .clone()
            .setActionId("act1")
            .setUserInfo(UserInfo.newBuilder().setUserId("userId1"))
            .setTiming(
                ai.promoted.proto.common.Timing.newBuilder()
                    .setEventApiTimestamp(EVENT_API_TIMESTAMP))
            .build();
    List<LogRequest> logRequests =
        ImmutableList.of(
            LogRequest.newBuilder().setTiming(action1.getTiming()).addAction(action1).build());

    job.startValidateAndEnrichJob(
        job.metricsApiKafkaSource.splitSources(
            fromItems(env, "logRequest", logRequests, LogRequest::getTiming)));
    waitForDone(env.execute("validate-enrich"));

    // Our test Kafka infra has issues so we'll write to files.
    File kafkaOutput = new File(tempDir, "kafka");
    assertThat(ImmutableSet.copyOf(kafkaOutput.list()))
        .containsExactlyElementsIn(
            ImmutableSet.of("metrics.default.action", "metrics.default.retained-user"));

    String expectedRetainedUserId = "4a104faa9f5a2e86a852ee989d8a6b85";

    String path = String.format("kafka/metrics.default.action/dt=%s/hour=%s", DT, HOUR);
    assertAvroFiles(
        toFixedAvroGenericRecords(
            listOf(
                action1.toBuilder()
                    .setTiming(action1.getTiming().toBuilder().setLogTimestamp(1601596151000L))
                    .setUserInfo(
                        action1.getUserInfo().toBuilder()
                            .clearUserId()
                            .setRetainedUserId(expectedRetainedUserId))
                    .build())),
        assertPartFilesExist(new File(tempDir, path)),
        "action");

    path = String.format("kafka/metrics.default.retained-user/dt=%s/hour=%s", DT, HOUR);
    assertAvroFiles(
        toFixedAvroGenericRecords(
            listOf(
                RetainedUser.newBuilder()
                    .setPlatformId(1L)
                    .setUserId("userId1")
                    .setRetainedUserId(expectedRetainedUserId)
                    .setCreateEventApiTimeMillis(1601596151000L)
                    .setProcessTimeMillis(0L)
                    .build())),
        assertPartFilesExist(new File(tempDir, path)),
        "retained-user");

    // Verify the table is created.
    String catalogPath = new File(tempDir, "paimon/").getAbsolutePath();
    String createCatalogSql =
        String.format(
            "CREATE CATALOG paimon WITH ('warehouse'='%s', 'type'='paimon')", catalogPath);
    EnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build();
    TableEnvironment tableEnv = TableEnvironment.create(settings);
    tableEnv.executeSql(createCatalogSql);
    String dbName = TableUtil.getLabeledDatabaseName(DATABASE_NAME, job.getJobLabel());
    String createDatabaseSql = String.format("CREATE DATABASE IF NOT EXISTS `paimon`.`%s`", dbName);
    tableEnv.executeSql(createDatabaseSql);
    String fullTableName = TableUtil.fullTableName("paimon", dbName, "retained_user");
    String createTableSql = ValidateEnrichJob.getCreateRetainedUserPaimonTableSql(fullTableName);
    tableEnv.executeSql(createTableSql);
    Table table = tableEnv.sqlQuery(String.format("SELECT * FROM %s", fullTableName));
    Iterator<Row> result = table.execute().collect();
    // TODO - why is process timestamp null?  I'd expect zero.
    String expectedRetainedUserStr =
        String.format("+I[1, userId1, %s, 1601596151000, null]", expectedRetainedUserId);
    assertThat(result.next().toString()).isEqualTo(expectedRetainedUserStr);
  }

  // Before turning on retainedUserId
  @Test
  void authUserId_retainedUserIdDisabled() throws Exception {
    ValidateEnrichJob job = createJob();
    job.kafkaSourceSegment.startFromEarliest = true;
    // Created in setup.
    job.anonymizerSegment.staticKeyAnonymizer.remove("retained_user_id");

    S3Segment s3 = new S3Segment(job);
    s3.rootPath = tempDir.getAbsolutePath();
    S3FileOutput s3FileOutput = new S3FileOutput(job, s3);
    job.validatedEventSink = new MockValidatedEventSink(s3FileOutput);
    runAuthUserIdTest(
        job, ImmutableSet.of("metrics.default.action", "metrics.default.log-user-user-event"), "");
  }

  private void runAuthUserIdTest(
      ValidateEnrichJob job, Set<String> expectedKafkaOutputs, String expectedRetainedUserId)
      throws Exception {
    Action.Builder actionBuilder =
        Action.newBuilder()
            .setPlatformId(1L)
            .setTiming(
                ai.promoted.proto.common.Timing.newBuilder()
                    .setEventApiTimestamp(EVENT_API_TIMESTAMP))
            .setContentId("content1");
    Action action1 =
        actionBuilder
            .clone()
            .setActionId("act1")
            .setUserInfo(UserInfo.newBuilder().setAnonUserId("anonUserId1").setUserId("userId1"))
            .setTiming(
                ai.promoted.proto.common.Timing.newBuilder()
                    .setEventApiTimestamp(EVENT_API_TIMESTAMP))
            .build();
    Action action2 =
        actionBuilder
            .clone()
            .setActionId("act2")
            .setUserInfo(UserInfo.newBuilder().setAnonUserId("anonUserId2"))
            .setTiming(
                ai.promoted.proto.common.Timing.newBuilder()
                    .setEventApiTimestamp(EVENT_API_TIMESTAMP + 1))
            .build();
    Action action3 =
        actionBuilder
            .clone()
            .setActionId("act3")
            .setUserInfo(UserInfo.newBuilder().setAnonUserId("anonUserId2").setUserId("userId1"))
            .setTiming(
                ai.promoted.proto.common.Timing.newBuilder()
                    .setEventApiTimestamp(EVENT_API_TIMESTAMP + 2))
            .build();
    List<LogRequest> logRequests =
        ImmutableList.of(
            LogRequest.newBuilder().setTiming(action1.getTiming()).addAction(action1).build(),
            LogRequest.newBuilder().setTiming(action2.getTiming()).addAction(action2).build(),
            LogRequest.newBuilder().setTiming(action3.getTiming()).addAction(action3).build());

    job.startValidateAndEnrichJob(
        job.metricsApiKafkaSource.splitSources(
            fromItems(env, "logRequest", logRequests, LogRequest::getTiming)));
    waitForDone(env.execute("validate-enrich"));

    // Our test Kafka infra has issues so we'll write to files.
    File kafkaOutput = new File(tempDir, "kafka");
    assertThat(ImmutableSet.copyOf(kafkaOutput.list()))
        .containsExactlyElementsIn(expectedKafkaOutputs);

    String path = String.format("kafka/metrics.default.action/dt=%s/hour=%s", DT, HOUR);
    assertAvroFiles(
        toFixedAvroGenericRecords(
            listOf(
                action1.toBuilder()
                    .setTiming(action1.getTiming().toBuilder().setLogTimestamp(1601596151000L))
                    .setUserInfo(
                        action1.getUserInfo().toBuilder()
                            .clearUserId()
                            .setAnonUserId("anonuserid1")
                            .setLogUserId("anonuserid1")
                            .setRetainedUserId(expectedRetainedUserId))
                    .build(),
                // Since events are outputted on a timer, even this one is changed.
                action2.toBuilder()
                    .setTiming(action2.getTiming().toBuilder().setLogTimestamp(1601596151001L))
                    .setUserInfo(
                        action2.getUserInfo().toBuilder()
                            .clearUserId()
                            .setAnonUserId("anonuserid2")
                            .setLogUserId("anonuserid2"))
                    .build(),
                // back to the first logUserId since userId is recognized.
                action3.toBuilder()
                    .setTiming(action3.getTiming().toBuilder().setLogTimestamp(1601596151002L))
                    .setUserInfo(
                        action3.getUserInfo().toBuilder()
                            .clearUserId()
                            .setAnonUserId("anonuserid2")
                            .setLogUserId("anonuserid2")
                            .setRetainedUserId(expectedRetainedUserId))
                    .build())),
        assertPartFilesExist(new File(tempDir, path)),
        "action");

    path = String.format("kafka/metrics.default.log-user-user-event/dt=%s/hour=%s", DT, HOUR);
    assertAvroParquetFiles(
        listOf(
            LogUserUser.newBuilder()
                .setPlatformId(1L)
                .setLogUserId("anonuserid1")
                .setUserId("userId1")
                .setEventTimeMillis(1601596151000L)
                .build(),
            LogUserUser.newBuilder()
                .setPlatformId(1L)
                .setLogUserId("anonuserid2")
                .setUserId("userId1")
                .setEventTimeMillis(1601596151002L)
                .build()),
        assertPartFilesExist(new File(tempDir, path)),
        "log-user-user");

    if (!expectedRetainedUserId.isEmpty()) {
      path =
          String.format(
              "kafka/metrics.default.anon-user-retained-user-event/dt=%s/hour=%s", DT, HOUR);
      assertAvroFiles(
          toFixedAvroGenericRecords(
              listOf(
                  AnonUserRetainedUser.newBuilder()
                      .setPlatformId(1L)
                      .setAnonUserId("anonuserid1")
                      .setRetainedUserId(expectedRetainedUserId)
                      .setEventTimeMillis(1601596151000L)
                      .build(),
                  AnonUserRetainedUser.newBuilder()
                      .setPlatformId(1L)
                      .setAnonUserId("anonuserid2")
                      .setRetainedUserId(expectedRetainedUserId)
                      .setEventTimeMillis(1601596151002L)
                      .build())),
          assertPartFilesExist(new File(tempDir, path)),
          "anon-user-retained-user");
    }
  }

  private void copyLogToAnonUserId(UserInfo.Builder builder) {
    builder.setAnonUserId(builder.getLogUserId());
  }

  private void copyAnonToLogUserId(UserInfo.Builder builder) {
    builder.setLogUserId(builder.getAnonUserId());
  }

  private <T> ImmutableList<T> listOf(T... values) {
    return ImmutableList.copyOf(values);
  }

  private <T> T getOnly(List<T> values) {
    Preconditions.checkArgument(
        values.size() == 1, "Expected only one value.  Hit %s", values.size());
    return values.get(0);
  }

  private CohortMembership getOnlyCohortMembership(List<LogRequest> logRequests) {
    return getOnly(LogRequestFactory.pushDownToCohortMemberships(logRequests));
  }

  private View getOnlyView(List<LogRequest> logRequests) {
    return getOnly(LogRequestFactory.pushDownToViews(logRequests));
  }

  private DeliveryLog getOnlyDeliveryLog(List<LogRequest> logRequests) {
    return getOnly(LogRequestFactory.pushDownToDeliveryLogs(logRequests));
  }

  private Impression getOnlyImpression(List<LogRequest> logRequests) {
    return getOnly(LogRequestFactory.pushDownToImpressions(logRequests));
  }

  private Action getOnlyAction(List<LogRequest> logRequests) {
    return getOnly(LogRequestFactory.pushDownToActions(logRequests));
  }

  private Diagnostics getOnlyDiagnostics(List<LogRequest> logRequests) {
    return getOnly(LogRequestFactory.pushDownToDiagnostics(logRequests));
  }
}
