package ai.promoted.metrics.logprocessor.job.raw;

import static ai.promoted.metrics.logprocessor.common.testing.AvroAsserts.assertAvroFiles;
import static ai.promoted.metrics.logprocessor.common.testing.AvroAsserts.assertAvroParquetFiles;
import static ai.promoted.metrics.logprocessor.common.testing.AvroProtoUtils.toFixedAvroGenericRecords;
import static ai.promoted.metrics.logprocessor.common.testing.FileAsserts.assertHasAnyFile;
import static ai.promoted.metrics.logprocessor.common.testing.FileAsserts.assertPartFilesExist;
import static ai.promoted.metrics.logprocessor.common.util.TableUtil.getLabeledDatabaseName;
import static com.google.common.truth.Truth.assertThat;

import ai.promoted.metrics.common.DeliveryLogIds;
import ai.promoted.metrics.common.ExecutionServer;
import ai.promoted.metrics.common.LogUserUser;
import ai.promoted.metrics.common.RequestInsertionIds;
import ai.promoted.metrics.common.ResponseInsertionIds;
import ai.promoted.metrics.logprocessor.common.fakedatagenerator.LogRequestFactory;
import ai.promoted.metrics.logprocessor.common.job.testing.BaseJobMiniclusterTest;
import ai.promoted.metrics.logprocessor.common.testing.MiniClusterExtension;
import ai.promoted.metrics.logprocessor.common.util.TableUtil;
import ai.promoted.metrics.logprocessor.common.util.TrackingUtil;
import ai.promoted.proto.common.AnonUserRetainedUser;
import ai.promoted.proto.common.Timing;
import ai.promoted.proto.delivery.DeliveryLog;
import ai.promoted.proto.event.Action;
import ai.promoted.proto.event.CohortMembership;
import ai.promoted.proto.event.Diagnostics;
import ai.promoted.proto.event.Impression;
import ai.promoted.proto.event.LogRequest;
import ai.promoted.proto.event.View;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import java.io.File;
import java.time.Duration;
import java.util.Collections;
import java.util.Iterator;
import java.util.List;
import java.util.Optional;
import java.util.regex.Pattern;
import org.apache.flink.streaming.api.graph.StreamGraph;
import org.apache.flink.streaming.api.graph.StreamNode;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.TableEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.types.Row;
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.testcontainers.junit.jupiter.Testcontainers;

/**
 * Minicluster test for RawOutputJob. The DeliveryLog test contains more assertions.
 *
 * <p>Warning: this test runs the stream jobs as batch. Flink stream minicluster tests don't
 * automatically checkpoint at the end. TODO - add link to thread about this.
 */
@ExtendWith(MiniClusterExtension.class)
@Testcontainers
public class RawOutputJobMiniclusterTest extends BaseJobMiniclusterTest<RawOutputJob> {
  private static final String DATABASE_NAME = "raw_db";
  private static final String DT = "2020-10-01";
  private static final String HOUR = "23";
  private static final long EVENT_API_TIMESTAMP = 1601596151000L;

  private static List<LogRequest> createLogRequests() {
    return LogRequestFactory.createLogRequests(
        LogRequestFactory.createLogRequestOptionsBuilder(
            EVENT_API_TIMESTAMP, LogRequestFactory.DetailLevel.PARTIAL, 1));
  }

  @Override
  protected RawOutputJob createJob() {
    RawOutputJob job = new RawOutputJob();
    job.jobLabel = "blue";
    job.s3.rootPath = tempDir.getAbsolutePath();
    // Checkpoint more frequently so we don't hit part files.
    job.checkpointInterval = Duration.ofSeconds(1);
    job.checkpointTimeout = Duration.ofSeconds(60);
    job.disableAutoGeneratedUIDs = false;
    job.configureExecutionEnvironment(env, 1, 0);
    TrackingUtil.processingTimeSupplier = () -> 0L;
    return job;
  }

  private RawOutputJob createPaimonJob() {
    RawOutputJob job = createJob();
    StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
    job.setTableEnv(tEnv);
    job.writePaimonTables = true;
    job.maxParallelism = 2;
    job.paimonSegment.paimonBasePath = tempDir.getAbsolutePath() + "/paimon/";
    job.paimonSegment.paimonCatalogName = "raw-tables";
    job.paimonSegment.paimonDefaultDatabaseName = DATABASE_NAME;
    TrackingUtil.processingTimeSupplier = () -> 1L;
    return job;
  }

  // TODO - change one of the tests to test the full job setup.  These currently tests parts
  // separately.

  // TODO - tests for LogUserUser.

  @Test
  void testCohortMembership_AvroOutput() throws Exception {
    List<CohortMembership> cohortMemberships =
        LogRequestFactory.pushDownToCohortMemberships(createLogRequests());
    RawOutputJob job = createJob();

    job.outputCohortMembership(
        getLabeledDatabaseName(DATABASE_NAME, job.getJobLabel()),
        "cohort_membership",
        fromItems(env, "cohort-membership", cohortMemberships, CohortMembership::getTiming));

    waitForDone(env.execute("log-log-request"));

    String path = String.format("blue/raw/cohort_membership/dt=%s/hour=%s", DT, HOUR);
    assertAvroFiles(
        toFixedAvroGenericRecords(cohortMemberships),
        assertPartFilesExist(new File(tempDir, path)),
        "raw/cohort_membership");
  }

  @Test
  void testView_AvroOutput() throws Exception {
    List<View> views = LogRequestFactory.pushDownToViews(createLogRequests());
    RawOutputJob job = createJob();
    job.outputView(
        getLabeledDatabaseName(DATABASE_NAME, job.getJobLabel()),
        "view",
        fromItems(env, "view", views, View::getTiming));

    waitForDone(env.execute("log-log-request"));

    String path = String.format("blue/raw/view/dt=%s/hour=%s", DT, HOUR);
    assertAvroFiles(
        toFixedAvroGenericRecords(views),
        assertPartFilesExist(new File(tempDir, path)),
        "raw/view");
  }

  // TODO - add AutoView tests.
  @Test
  void testPaimonOutput() throws Exception {
    List<View> views = LogRequestFactory.pushDownToViews(createLogRequests());
    List<DeliveryLog> deliveryLogs = LogRequestFactory.pushDownToDeliveryLogs(createLogRequests());
    List<Diagnostics> diagnostics = LogRequestFactory.pushDownToDiagnostics(createLogRequests());
    List<LogUserUser> logUserUsers =
        Collections.singletonList(
            LogUserUser.newBuilder()
                .setUserId("user_id")
                .setLogUserId("log_user_id")
                .setEventTimeMillis(EVENT_API_TIMESTAMP)
                .setPlatformId(3)
                .build());

    RawOutputJob job = createPaimonJob();
    job.partitionByHour = true;
    job.tablePartitionByHour =
        ImmutableMap.of(
            TableUtil.databaseAndTableName(DATABASE_NAME, "view"),
            false,
            TableUtil.databaseAndTableName(DATABASE_NAME, "log_user_user"),
            false);
    // Set the writer parallelism for delivery_log table with operator parallelism setting
    job.operatorParallelism = ImmutableMap.of("Writer : delivery_log", 2);
    // Set the writer parallelism for view table with Paimon option
    job.paimonSegment.paimonTableOptions =
        ImmutableMap.of(DATABASE_NAME + ".view.sink.parallelism", "2");
    job.paimonSegment.paimonAlterTable = ImmutableMap.of(DATABASE_NAME + ".delivery_log", true);
    job.outputView(DATABASE_NAME, "view", fromItems(env, "view", views, View::getTiming));
    job.outputDeliveryLog(
        DATABASE_NAME,
        "delivery_log",
        fromItems(env, "delivery-log", deliveryLogs, record -> record.getRequest().getTiming()));
    // Diagnostics doesn't have a PK
    job.outputDiagnostics(
        DATABASE_NAME,
        "diagnostics",
        fromItems(env, "diagnostics", diagnostics, Diagnostics::getTiming));
    job.outputLogUserUser(
        DATABASE_NAME,
        "log_user_user",
        fromItems(
            env,
            "log-user-user",
            logUserUsers,
            record ->
                Timing.newBuilder().setEventApiTimestamp(record.getEventTimeMillis()).build()));

    job.prepareToExecute();
    StreamGraph streamGraph = env.getStreamGraph(true);
    Optional<StreamNode> writeDeliveryNode =
        streamGraph.getStreamNodes().stream()
            // For nodes other than sources and sinks, the name is equal to the transformation name
            .filter(node -> node.getOperatorName().equals("Writer : delivery_log"))
            .findFirst();
    Assertions.assertTrue(writeDeliveryNode.isPresent());
    Assertions.assertEquals(2, writeDeliveryNode.get().getParallelism());
    Optional<StreamNode> writeViewNode =
        streamGraph.getStreamNodes().stream()
            // For nodes other than sources and sinks, the name is equal to the transformation name
            .filter(node -> node.getOperatorName().equals("Writer : view"))
            .findFirst();
    Assertions.assertTrue(writeViewNode.isPresent());
    Assertions.assertEquals(2, writeViewNode.get().getParallelism());
    waitForDone(env.execute(streamGraph));

    String path =
        String.format(
            "paimon/%s.db/%s/%s/bucket-0",
            getLabeledDatabaseName(DATABASE_NAME, job.getJobLabel()), "view", "dt=" + DT);
    assertHasAnyFile(new File(tempDir, path), Pattern.compile(".+.(orc|parquet|avro)"));
    path =
        String.format(
            "paimon/%s.db/%s/%s/%s/bucket-0",
            getLabeledDatabaseName(DATABASE_NAME, job.getJobLabel()),
            "diagnostics",
            "dt=" + DT,
            "hr=" + HOUR);
    assertHasAnyFile(new File(tempDir, path), Pattern.compile(".+.(orc|parquet|avro)"));
    String labeledDatabase = getLabeledDatabaseName(DATABASE_NAME, job.getJobLabel());
    path = String.format("paimon/%s.db/%s/bucket-0", labeledDatabase, "log_user_user");
    assertHasAnyFile(new File(tempDir, path), Pattern.compile(".+.(orc|parquet|avro)"));

    // Validate the content of the delivery_log table
    String catalogPath = new File(tempDir, "paimon/").getAbsolutePath();
    String createCatalogSql =
        String.format(
            "CREATE CATALOG paimon WITH ('warehouse'='%s', 'type'='paimon')", catalogPath);
    EnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build();
    TableEnvironment tableEnv = TableEnvironment.create(settings);
    tableEnv.executeSql(createCatalogSql);
    Table table =
        tableEnv.sqlQuery(
            String.format(
                "SELECT request FROM paimon.%s.%s WHERE dt >= '%s'",
                labeledDatabase, "delivery_log", DT));
    Iterator<Row> result = table.execute().collect();
    // Only have one delivery log
    String expectedDeliveryLogStr =
        "+I[+I[1, +I[null, 00000000-0000-0000-0000-000000000001, null, null, null, null], +I[1601596151000, 1601596151000, null, 1], +I[PLATFORM_SERVER, PRODUCTION], null, 33333333-3333-3333-0000-000000000001, 22222222-2222-2222-0000-000000000001, null, null, client-33333333-3333-3333-0000-000000000001, null, null, null, [+I[null, null, null, null, null, null, null, null, null, i-1-1, null, +I[null, +I[[+I[itemId, +I[null, null, i-1-1, null, null, null]], +I[storeId, +I[null, null, s-1-1, null, null, null]], +I[type, +I[null, null, ITEM, null, null, null]]]], null], null, null, null, null, null, null]], null, null, null, null, null]]";
    assertThat(result.next().toString()).isEqualTo(expectedDeliveryLogStr);
  }

  @Test
  void testPaimonOutput_AnonUserRetainedUser() throws Exception {
    List<AnonUserRetainedUser> anonUserRetainedUser =
        Collections.singletonList(
            AnonUserRetainedUser.newBuilder()
                .setRetainedUserId("retainedUserId1")
                .setAnonUserId("anonUserId1")
                .setEventTimeMillis(EVENT_API_TIMESTAMP)
                .setPlatformId(3)
                .build());

    RawOutputJob job = createPaimonJob();
    job.partitionByHour = true;
    job.tablePartitionByHour =
        ImmutableMap.of(
            TableUtil.databaseAndTableName(DATABASE_NAME, "anon_user_retained_user"), false);
    // Set the writer parallelism for delivery_log table with operator parallelism setting
    job.operatorParallelism = ImmutableMap.of();
    // Set the writer parallelism for view table with Paimon option
    job.paimonSegment.paimonTableOptions = ImmutableMap.of();
    job.paimonSegment.paimonAlterTable = ImmutableMap.of();
    job.outputAnonUserRetainedUser(
        DATABASE_NAME,
        "anon_user_retained_user",
        fromItems(
            env,
            "anon-user-retained-user",
            AnonUserRetainedUser::getEventTimeMillis,
            anonUserRetainedUser));

    job.prepareToExecute();
    StreamGraph streamGraph = env.getStreamGraph(true);
    waitForDone(env.execute(streamGraph));

    String labeledDatabase = getLabeledDatabaseName(DATABASE_NAME, job.getJobLabel());
    String path =
        String.format("paimon/%s.db/%s/bucket-0", labeledDatabase, "anon_user_retained_user");
    assertHasAnyFile(new File(tempDir, path), Pattern.compile(".+.(orc|parquet|avro)"));

    // Validate the contents.
    String catalogPath = new File(tempDir, "paimon/").getAbsolutePath();
    String createCatalogSql =
        String.format(
            "CREATE CATALOG paimon WITH ('warehouse'='%s', 'type'='paimon')", catalogPath);
    EnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build();
    TableEnvironment tableEnv = TableEnvironment.create(settings);
    tableEnv.executeSql(createCatalogSql);
    Table table =
        tableEnv.sqlQuery(
            String.format(
                "SELECT * FROM paimon.%s.%s", labeledDatabase, "anon_user_retained_user"));
    Iterator<Row> result = table.execute().collect();
    // Only have one AnonUserRetainedUser
    String expectedAnonUserRetainedUserStr =
        "+I[3, anonUserId1, retainedUserId1, 1601596151000, null]";
    assertThat(result.next().toString()).isEqualTo(expectedAnonUserRetainedUserStr);
  }

  @Test
  void testDeliveryLog_AvroOutput() throws Exception {
    List<DeliveryLog> deliveryLogs = LogRequestFactory.pushDownToDeliveryLogs(createLogRequests());
    RawOutputJob job = createJob();

    job.outputDeliveryLogTables(
        fromItems(env, "delivery-log", deliveryLogs, record -> record.getRequest().getTiming()));

    waitForDone(env.execute("log-log-request"));

    String path = String.format("blue/raw/delivery_log/dt=%s/hour=%s", DT, HOUR);
    assertAvroFiles(
        toFixedAvroGenericRecords(deliveryLogs),
        assertPartFilesExist(new File(tempDir, path)),
        "raw/delivery_log");

    path = String.format("blue/raw-side/delivery-log-ids/dt=%s/hour=%s", DT, HOUR);
    File[] parquetFiles = assertPartFilesExist(new File(tempDir, path));
    assertAvroParquetFiles(
        ImmutableList.of(
            DeliveryLogIds.newBuilder()
                .setPlatformId(deliveryLogs.get(0).getPlatformId())
                .setAnonUserId(deliveryLogs.get(0).getRequest().getUserInfo().getAnonUserId())
                .setEventApiTimestamp(EVENT_API_TIMESTAMP)
                .setExecutionServer(ExecutionServer.SDK)
                .setViewId("22222222-2222-2222-0000-000000000001")
                .setRequestId("33333333-3333-3333-0000-000000000001")
                .setClientRequestId("client-33333333-3333-3333-0000-000000000001")
                .setSearchQuery("")
                .setUserAgent("")
                .build()),
        parquetFiles,
        "blue/raw-side/delivery-log-ids");

    path = String.format("blue/raw-side/request-insertion-ids/dt=%s/hour=%s", DT, HOUR);
    parquetFiles = assertPartFilesExist(new File(tempDir, path));
    assertAvroParquetFiles(
        ImmutableList.of(
            RequestInsertionIds.newBuilder()
                .setPlatformId(deliveryLogs.get(0).getPlatformId())
                .setEventApiTimestamp(EVENT_API_TIMESTAMP)
                .setRequestId("33333333-3333-3333-0000-000000000001")
                .setContentId("i-1-1")
                .setRetrievalRank(null)
                .build()),
        parquetFiles,
        "blue/raw-side/request-insertion-ids");

    path = String.format("blue/raw-side/response-insertion-ids/dt=%s/hour=%s", DT, HOUR);
    parquetFiles = assertPartFilesExist(new File(tempDir, path));
    assertAvroParquetFiles(
        ImmutableList.of(
            ResponseInsertionIds.newBuilder()
                .setPlatformId(deliveryLogs.get(0).getPlatformId())
                .setEventApiTimestamp(EVENT_API_TIMESTAMP)
                .setRequestId("33333333-3333-3333-0000-000000000001")
                .setInsertionId("44444444-4444-4444-0000-000000000001")
                .setContentId("i-1-1")
                .setPosition(0L)
                .build()),
        parquetFiles,
        "blue/raw-side/request-insertion-ids");
  }

  @Test
  void testImpression_AvroOutput() throws Exception {
    List<Impression> impressions = LogRequestFactory.pushDownToImpressions(createLogRequests());
    RawOutputJob job = createJob();

    job.outputImpression(
        getLabeledDatabaseName(DATABASE_NAME, job.getJobLabel()),
        "impression",
        fromItems(env, "impression", impressions, Impression::getTiming));

    waitForDone(env.execute("log-log-request"));

    String path = String.format("blue/raw/impression/dt=%s/hour=%s", DT, HOUR);
    assertAvroFiles(
        toFixedAvroGenericRecords(impressions),
        assertPartFilesExist(new File(tempDir, path)),
        "raw/impression");
  }

  @Test
  void testAction_AvroOutput() throws Exception {
    List<Action> actions = LogRequestFactory.pushDownToActions(createLogRequests());
    RawOutputJob job = createJob();

    job.outputAction(
        getLabeledDatabaseName(DATABASE_NAME, job.getJobLabel()),
        "action",
        fromItems(env, "action", actions, Action::getTiming));

    waitForDone(env.execute("log-log-request"));

    String path = String.format("blue/raw/action/dt=%s/hour=%s", DT, HOUR);
    assertAvroFiles(
        toFixedAvroGenericRecords(actions),
        assertPartFilesExist(new File(tempDir, path)),
        "raw/action");
  }

  @Test
  void testDiagnostics_AvroOutput() throws Exception {
    List<Diagnostics> diagnostics = LogRequestFactory.pushDownToDiagnostics(createLogRequests());
    RawOutputJob job = createJob();

    job.outputDiagnostics(
        getLabeledDatabaseName(DATABASE_NAME, job.getJobLabel()),
        "diagnostics",
        fromItems(env, "diagnostics", diagnostics, Diagnostics::getTiming));

    waitForDone(env.execute("log-log-request"));

    String path = String.format("blue/raw/diagnostics/dt=%s/hour=%s", DT, HOUR);
    assertAvroFiles(
        toFixedAvroGenericRecords(diagnostics),
        assertPartFilesExist(new File(tempDir, path)),
        "raw/diagnostics");
  }

  // TODO - add a different duplicates test.
}
