package ai.promoted.metrics.logprocessor.job.fixschema;

import static ai.promoted.metrics.logprocessor.common.testing.AvroAsserts.loadAvroRecords;
import static ai.promoted.metrics.logprocessor.common.testing.FileAsserts.assertPartFilesExist;
import static org.junit.jupiter.api.Assertions.assertEquals;

import ai.promoted.metrics.datafix.RecursiveValue;
import ai.promoted.metrics.datafix.TestRecord1;
import ai.promoted.metrics.logprocessor.common.job.testing.BaseJobMiniclusterTest;
import ai.promoted.metrics.logprocessor.common.testing.MiniClusterExtension;
import java.io.File;
import java.io.IOException;
import java.time.Duration;
import java.util.Set;
import org.apache.avro.file.DataFileWriter;
import org.apache.avro.generic.GenericRecord;
import org.apache.avro.io.DatumWriter;
import org.apache.avro.specific.SpecificDatumWriter;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;

@ExtendWith(MiniClusterExtension.class)
public class FixSchemaJobMiniclusterTest extends BaseJobMiniclusterTest<FixSchemaJob> {

  @Override
  protected FixSchemaJob createJob() {
    FixSchemaJob job = new FixSchemaJob();
    // TODO - fix this.
    job.disableAutoGeneratedUIDs = false;
    job.s3.rootPath = tempDir.getAbsolutePath();
    // Checkpoint more frequently so we don't hit part files.
    job.checkpointInterval = Duration.ofSeconds(1);
    job.checkpointTimeout = Duration.ofSeconds(1);
    job.configureExecutionEnvironment(env, 1, 0);
    return job;
  }

  @Test
  void testSimpleCase() throws Exception {
    TestRecord1 record =
        TestRecord1.newBuilder()
            .setPlatformId(1L)
            // GMT: Thursday, October 1, 2020 11:49:11 PM
            .setEventApiTimestamp(1601596151000L)
            .setValue(
                RecursiveValue.newBuilder()
                    .setValue(RecursiveValue.newBuilder().setStringValue("b").build())
                    .setStringValue("b")
                    .build())
            .build();

    writeTestData(record);

    FixSchemaJob job = createJob();
    job.schema = "TestRecord2";
    job.prefix = "test/record";
    job.startDt = "2020-09-30";
    job.endDt = "2020-10-02";
    job.outputPrefix = "fix/record";

    job.fixSchema(env);
    waitForDone(env.execute("fix-schema"));

    String path = "fix/record/dt=2020-10-01/hour=23";
    File[] fixedAvroFiles = assertPartFilesExist(new File(tempDir, path));
    assertEquals(1, fixedAvroFiles.length);

    Set<GenericRecord> actualRecords = loadAvroRecords(fixedAvroFiles);
    assertEquals(1, actualRecords.size());
    GenericRecord actualRecord = actualRecords.iterator().next();
    actualRecord.getSchema().toString().contains("newTestField");
    assertEquals(1L, actualRecord.get("platformId"));
    assertEquals(0L, actualRecord.get("newTestField"));
    assertEquals(1601596151000L, actualRecord.get("eventApiTimestamp"));
  }

  private void writeTestData(TestRecord1 record) throws IOException {
    DatumWriter<TestRecord1> writer = new SpecificDatumWriter<>(TestRecord1.class);
    File dataDir = new File(tempDir, "test/record/dt=2020-10-01/hour=23");
    dataDir.mkdirs();
    File data = new File(dataDir, "part-123");
    try (DataFileWriter<TestRecord1> fileWriter = new DataFileWriter<>(writer)) {
      fileWriter.create(TestRecord1.SCHEMA$, data);
      fileWriter.append(record);
    }
  }
}
