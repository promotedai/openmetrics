package ai.promoted.metrics.logprocessor.job.contentetl;

import static ai.promoted.metrics.logprocessor.common.util.TableUtil.getLabeledDatabaseName;
import static ai.promoted.metrics.logprocessor.job.contentetl.ContentEtlJob.WriteMode.OVERWRITE;

import ai.promoted.metrics.logprocessor.common.job.testing.BaseJobMiniclusterTest;
import ai.promoted.metrics.logprocessor.common.testing.MiniClusterExtension;
import ai.promoted.metrics.logprocessor.job.contentetl.ContentEtlJob.ContentType;
import ai.promoted.metrics.logprocessor.job.contentetl.ContentEtlJob.SourceType;
import java.io.File;
import java.util.ArrayList;
import java.util.List;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.TableEnvironment;
import org.apache.flink.table.api.TableResult;
import org.apache.flink.types.Row;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;

@ExtendWith(MiniClusterExtension.class)
public class ContentEtlJobMiniclusterTest extends BaseJobMiniclusterTest<ContentEtlJob> {
  public static final Logger LOGGER = LogManager.getLogger(ContentEtlJobMiniclusterTest.class);

  @Test
  void testContentEtl() throws Exception {
    ContentEtlJob job = createJob();
    String path = "pipeline/src/test/java/resources/content/content/";
    job.contentSourcePath = new File(path).getAbsolutePath();
    job.contentType = ContentType.contents;
    job.sourceType = SourceType.json;
    TableResult result = job.contentEtl(env);
    result.await();
    verifyPaimonTableRowCount(job, 4);

    path = "pipeline/src/test/java/resources/contentfullload/content/";
    job.contentSourcePath = new File(path).getAbsolutePath();
    job.sourceType = SourceType.parquet;

    job.writeMode = OVERWRITE;
    job.contentEtl(env).await();
    verifyPaimonTableRowCount(job, 4);
  }

  @Test
  void testUserEtl() throws Exception {
    ContentEtlJob job = createJob();
    String path = "pipeline/src/test/java/resources/content/user/";
    job.contentSourcePath = new File(path).getAbsolutePath();
    job.contentType = ContentType.users;

    // write 2 rows in upsert mode
    job.contentEtl(env).await();
    verifyPaimonTableRowCount(job, 2);
  }

  private void verifyPaimonTableRowCount(ContentEtlJob job, int expectedRowCount) {
    EnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build();
    TableEnvironment tableEnv = TableEnvironment.create(settings);
    tableEnv.executeSql(
        "CREATE CATALOG `"
            + job.paimonSegment.paimonCatalogName
            + "` WITH ("
            + "'type' = 'paimon',"
            + "'warehouse' = '"
            + job.paimonSegment.paimonBasePath
            + "',"
            + "'default-database' = '"
            + getLabeledDatabaseName(job.databaseName, job.getJobLabel())
            + "')");
    Table table = tableEnv.sqlQuery("SELECT last_updated, dt, hr FROM " + job.fullTableName());
    List<Row> rows = new ArrayList<>();
    table.execute().collect().forEachRemaining(rows::add);
    Assertions.assertEquals(
        expectedRowCount,
        rows.size(),
        () ->
            String.format(
                "Should read %s rows from the Paimon table, but got %s", expectedRowCount, rows));
  }

  @Override
  protected ContentEtlJob createJob() {
    ContentEtlJob job = new ContentEtlJob();
    job.jobLabel = "blue";
    job.disableAutoGeneratedUIDs = false;
    job.paimonSegment.paimonCatalogName = "paimon_catalog";
    job.databaseName = "content";
    job.paimonSegment.paimonBasePath = tempDir.getAbsolutePath() + "/paimon";
    job.paimonTablePrefix = "content_paimon";
    job.configureExecutionEnvironment(env, 2, 10);
    return job;
  }
}
